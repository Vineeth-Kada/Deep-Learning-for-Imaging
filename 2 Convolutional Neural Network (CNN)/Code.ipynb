{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b05b3c1e",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf0f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import utils\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "img_dir = \"Images/PA2_\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40689597",
   "metadata": {},
   "source": [
    "# Utility Functions\n",
    "\n",
    "Code taken from tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3da1334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pbar(p=0, msg=\"\", bar_len=20):\n",
    "    sys.stdout.write(\"\\033[K\")\n",
    "    sys.stdout.write(\"\\x1b[2K\" + \"\\r\")\n",
    "    block = int(round(bar_len * p))\n",
    "    text = \"Progress: [{}] {}% {}\".format(\n",
    "        \"\\x1b[32m\" + \"=\" * (block - 1) + \">\" + \"\\033[0m\" + \"-\" * (bar_len - block),\n",
    "        round(p * 100, 2),\n",
    "        msg,\n",
    "    )\n",
    "    print(text, end=\"\\r\")\n",
    "    if p == 1:\n",
    "        print()\n",
    "\n",
    "class AvgMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.metrics = {}\n",
    "\n",
    "    def add(self, batch_metrics):\n",
    "        if self.metrics == {}:\n",
    "            for key, value in batch_metrics.items():\n",
    "                self.metrics[key] = [value]\n",
    "        else:\n",
    "            for key, value in batch_metrics.items():\n",
    "                self.metrics[key].append(value)\n",
    "\n",
    "    def get(self):\n",
    "        return {key: np.mean(value) for key, value in self.metrics.items()}\n",
    "\n",
    "    def msg(self):\n",
    "        avg_metrics = {key: np.mean(value) for key, value in self.metrics.items()}\n",
    "        return \"\".join([\"[{}] {:.5f} \".format(key, value) for key, value in avg_metrics.items()])\n",
    "\n",
    "def train(model, optim, lr_sched=None, epochs=200, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), criterion=None, metric_meter=None, out_dir=\"out/\"):\n",
    "  model.to(device)\n",
    "  best_acc = 0\n",
    "  for epoch in range(epochs):\n",
    "    model.train()\n",
    "    metric_meter.reset()\n",
    "    for indx, (img, target) in enumerate(train_loader):\n",
    "      img = img.to(device)\n",
    "      target = target.to(device)\n",
    "      \n",
    "      optim.zero_grad()\n",
    "      out = model.forward(img)\n",
    "      loss = criterion(out, target)\n",
    "      loss.backward()\n",
    "      optim.step()\n",
    "\n",
    "      metric_meter.add({\"train loss\": loss.item()})\n",
    "      pbar(indx / len(train_loader), msg=metric_meter.msg())\n",
    "    pbar(1, msg=metric_meter.msg())\n",
    "    train_loss_for_plot.append(metric_meter.get()[\"train loss\"])\n",
    "    \n",
    "    model.eval()\n",
    "    metric_meter.reset()\n",
    "    for indx, (img, target) in enumerate(val_loader):\n",
    "      img = img.to(device)\n",
    "      target = target.to(device)\n",
    "      out = model.forward(img)\n",
    "      loss = criterion(out, target)\n",
    "      acc = (out.argmax(1) == target).sum().item() * (100 / img.shape[0])\n",
    "\n",
    "      metric_meter.add({\"val loss\": loss.item(), \"val acc\": acc})\n",
    "      pbar(indx / len(val_loader), msg=metric_meter.msg())\n",
    "    pbar(1, msg=metric_meter.msg())\n",
    "    \n",
    "    val_metrics = metric_meter.get()\n",
    "    val_loss_for_plot.append(val_metrics[\"val loss\"])\n",
    "    val_acc_for_plot.append(val_metrics[\"val acc\"])\n",
    "    if val_metrics[\"val acc\"] > best_acc:\n",
    "      print(\n",
    "          \"\\x1b[33m\"\n",
    "          + f\"val acc improved from {round(best_acc, 5)} to {round(val_metrics['val acc'], 5)}\"\n",
    "          + \"\\033[0m\"\n",
    "      )\n",
    "      best_acc = val_metrics['val acc']\n",
    "      torch.save(model.state_dict(), os.path.join(out_dir, \"best.ckpt\"))\n",
    "    lr_sched.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d0d557",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5276e55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = datasets.MNIST('~/mnist_data', train=True, download=True, transform=transforms.ToTensor())\n",
    "data_test = datasets.MNIST('~/mnist_data', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc97e24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data_train[np.random.randint(len(data_train))][0][0], cmap='gray')\n",
    "print(len(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a0d1189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train data into train(80%) and validation(20%)\n",
    "# Also ensures that class representation remains same.\n",
    "\n",
    "train_indices, val_indices, _, _ = train_test_split(\n",
    "    range(len(data_train)),\n",
    "    data_train.targets,\n",
    "    stratify=data_train.targets, # Make sure that the percentage of each class is same in both train & val\n",
    "    test_size=int(0.2 * len(data_train)),\n",
    ")\n",
    "\n",
    "train_split = Subset(data_train, train_indices)\n",
    "val_split = Subset(data_train, val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f75cd335",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_split, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_split, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(data_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08131393",
   "metadata": {},
   "source": [
    "# Part 1: MNIST Classification using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "408627ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MyModel, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "    self.fc1   = nn.Linear(32 * 7 * 7, 500)\n",
    "    self.fc2   = nn.Linear(500, 10)\n",
    "    self.activ = nn.ReLU()\n",
    "    \n",
    "#     self.bn1    = nn.BatchNorm2d(32)\n",
    "#     self.bn2    = nn.BatchNorm2d(32)\n",
    "    \n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.activ(self.conv1(x))\n",
    "    out = self.pool1(out)\n",
    "#     out = self.bn1(out)\n",
    "    out = self.activ(self.conv2(out))\n",
    "    out = self.pool2(out)\n",
    "#     out = self.bn2(out)\n",
    "    out = nn.Flatten()(out)\n",
    "    out = self.activ(self.fc1(out))\n",
    "    out = self.fc2(out) # No need to take softmax here because CrossEntropyLoss\n",
    "                        # combines nn.LogSoftmax() and nn.NLLLoss() in one single class \"\"\"\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a6cb85",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79de235b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "epochs = 15\n",
    "optim = torch.optim.SGD(model.parameters(), lr=10**(-1.5), momentum=0.9, weight_decay=5e-4)\n",
    "lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=epochs)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "metric_meter = AvgMeter()\n",
    "# out_dir = \"MyModel_BatchNorm\"\n",
    "out_dir = \"MyModel\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "train_loss_for_plot = []\n",
    "val_loss_for_plot = []\n",
    "val_acc_for_plot = []\n",
    "\n",
    "train(model, optim, lr_sched, epochs=epochs, criterion=criterion, metric_meter=metric_meter, out_dir=out_dir)\n",
    "# After this the model will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7521710",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [i+1 for i in range(epochs)]\n",
    "\n",
    "plt.plot(X, train_loss_for_plot); plt.ylabel('Train Loss'); plt.xlabel('Epoch')\n",
    "plt.savefig(img_dir + \"train_loss.svg\"); plt.clf();\n",
    "\n",
    "plt.plot(X, val_loss_for_plot); plt.ylabel('Validation Loss'); plt.xlabel('Epoch')\n",
    "plt.savefig(img_dir + \"val_loss.svg\"); plt.clf();\n",
    "\n",
    "plt.plot([0] + X, [0] + val_acc_for_plot); plt.ylabel('Validation Accuracy'); plt.xlabel('Epoch')\n",
    "plt.savefig(img_dir + \"val_acc.svg\"); plt.clf();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74dd218",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2edded69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test accuracy is: 99.18391719745223\n"
     ]
    }
   ],
   "source": [
    "best_ckpt = torch.load(\"MyModel/best.ckpt\")\n",
    "# best_ckpt = torch.load(\"MyModel_BatchNorm/best.ckpt\")\n",
    "model_saved = MyModel()\n",
    "model_saved.load_state_dict(best_ckpt)\n",
    "model_saved.to(torch.device(\"cuda\"))\n",
    "\n",
    "l = []\n",
    "for indx, (img, target) in enumerate(test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    img = img.to(device)\n",
    "    target = target.to(device)\n",
    "    out = model_saved.forward(img)\n",
    "    acc = (out.argmax(1) == target).sum().item() * (100 / img.shape[0])\n",
    "    l.append(acc)\n",
    "print(\"Average test accuracy is:\", np.mean(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c47960",
   "metadata": {},
   "source": [
    "## Random Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fc6ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 7))\n",
    "for idx in range(10):\n",
    "    rand_sample = np.random.randint(len(data_test))\n",
    "    img = data_test[rand_sample][0][0]\n",
    "    act = str(data_test[rand_sample][1])\n",
    "    pred = str(model_saved.forward(img.view(1,1,28,28).to(device)).argmax(1).item())\n",
    "    plt.subplot(2, 5, idx+1)\n",
    "    plt.imshow(img, cmap='gray'); plt.axis('off'); plt.ioff()\n",
    "    plt.title('True: ' + act + '\\nPrediction: ' + pred, fontsize = 20, fontweight='bold', color = 'blue')\n",
    "plt.savefig(img_dir + 'random_ver.svg'); plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54646fab",
   "metadata": {},
   "source": [
    "# Part 2: Visualising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bda57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_saved)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afe7ac7",
   "metadata": {},
   "source": [
    "## Filter Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bfbbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://stackoverflow.com/a/55604568\n",
    "def visTensor(tensor, ch=0, allkernels=False, nrow=8, padding=1): \n",
    "    n,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(n*c, -1, w, h)\n",
    "    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=1)\n",
    "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "model_children = list(model_saved.children())\n",
    "filter1 = model_children[0].weight.data.clone().to(torch.device(\"cpu\"))\n",
    "filter2 = model_children[1].weight.data.clone().to(torch.device(\"cpu\"))\n",
    "\n",
    "visTensor(filter1); plt.axis('off'); plt.ioff()\n",
    "plt.savefig(img_dir + 'conv1_filters.png', dpi = 150); plt.clf()\n",
    "\n",
    "visTensor(filter2); plt.axis('off'); plt.ioff()\n",
    "plt.savefig(img_dir + 'conv2_filters.png', dpi = 150); plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b6a0182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImage(ACTUAL_CLASS):\n",
    "    image = None\n",
    "    for i in range(len(data_train)):\n",
    "        if(data_train[i][1] == ACTUAL_CLASS):\n",
    "            image = data_train[i][0].to(device)\n",
    "            break\n",
    "    assert(image != None)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692b5ddb",
   "metadata": {},
   "source": [
    "## Activation Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae3336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure(figsize = (5, 2))\n",
    "for ACTUAL_CLASS in range(10):\n",
    "    image = getImage(ACTUAL_CLASS).view(1, 1, 28, 28)\n",
    "\n",
    "    fmap1 = model_saved.activ(model_saved.conv1(image))\n",
    "    \n",
    "    plt.subplot(2, 5, ACTUAL_CLASS + 1)\n",
    "    visTensor(fmap1.to(torch.device('cpu'))); plt.axis('off'); plt.ioff()\n",
    "plt.savefig(img_dir + 'activ1.svg'); plt.clf()\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize = (5, 2))\n",
    "for ACTUAL_CLASS in range(10):\n",
    "    image = None\n",
    "    for i in range(len(data_train)):\n",
    "        if(data_train[i][1] == ACTUAL_CLASS):\n",
    "            image = data_train[i][0].to(device)\n",
    "            break\n",
    "    assert(image != None)\n",
    "    image = image.view(1, 1, 28, 28)\n",
    "\n",
    "\n",
    "    out = model_saved.activ(model_saved.conv1(image))\n",
    "    out = model_saved.pool1(out)\n",
    "    fmap2 = model_saved.activ(model_saved.conv2(out))\n",
    "    \n",
    "    plt.subplot(2, 5, ACTUAL_CLASS + 1)\n",
    "    \n",
    "    visTensor(fmap2.to(torch.device('cpu'))); plt.axis('off'); plt.ioff()\n",
    "plt.savefig(img_dir + 'activ2.svg'); plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b067d06",
   "metadata": {},
   "source": [
    "## Occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c57017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://www.kaggle.com/code/blargl/simple-occlusion-and-saliency-maps/notebook\n",
    "def occlusion_list(image, occ_size, occ_val = 0.5):\n",
    "    \n",
    "    assert(occ_size <= image.shape[0])\n",
    "    assert(occ_size % 2 == 1) # Only odd size occlusion\n",
    "    \n",
    "    occlusion = np.full((occ_size, occ_size), [occ_val], np.float32)\n",
    "    occ_list = []\n",
    "    \n",
    "    for x in range(occ_size, image.shape[0] + occ_size):\n",
    "        for y in range(occ_size, image.shape[1] + occ_size):\n",
    "            image_copy = np.pad(image, ((occ_size, occ_size), (occ_size, occ_size)), 'constant', constant_values = 0.0).copy()\n",
    "            \n",
    "            image_copy[x - occ_size // 2 : x + occ_size // 2 + 1, \\\n",
    "                       y - occ_size // 2 : y + occ_size // 2 + 1] = occlusion\n",
    "            \n",
    "            occ_list.append((x - occ_size, y - occ_size, image_copy[occ_size:occ_size+image.shape[0], occ_size:occ_size+image.shape[1]]))\n",
    "            \n",
    "    return occ_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978bc4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10\n",
    "\n",
    "np.random.seed(1234567)\n",
    "idx = [np.random.randint(len(data_train)) for _ in range(sample_size)]\n",
    "images = [data_train[idx[i]][0].detach().clone() for i in range(sample_size)]\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize=(8, 10))\n",
    "for i, img in enumerate(images):\n",
    "    plt.subplot(5, 4, 2 * i + 1)\n",
    "    plt.imshow(img[0], cmap='gray'); plt.axis('off'); plt.ioff();\n",
    "            \n",
    "    sz = img[0].shape[0]\n",
    "    heat_map = np.zeros((sz,sz))\n",
    "    occ_list = occlusion_list(img[0], occ_size = 7, occ_val = 0)\n",
    "    \n",
    "    for (x, y, img_occ) in occ_list:\n",
    "        img_occ = img_occ.reshape(1, 1, sz, sz)\n",
    "        img_occ = torch.Tensor(img_occ).to(device)\n",
    "        out = nn.Softmax(dim=1)(model_saved.forward(img_occ))\n",
    "        heat_map[x, y] = out[0, data_train[idx[i]][1]]\n",
    "\n",
    "    plt.subplot(5, 4, 2 * i + 2)\n",
    "    sn.heatmap(heat_map)\n",
    "\n",
    "plt.savefig(img_dir + 'occ_7.svg'); plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaec616",
   "metadata": {},
   "source": [
    "# Part 3: Adversial Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75c4595",
   "metadata": {},
   "source": [
    "## 3.1 Non Targeted Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95d7d97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MEAN = 0.5\n",
    "STD = 0.1\n",
    "\n",
    "gen_imgs = []\n",
    "cost_lists = []\n",
    "for TARGET_CLASS in range(10):\n",
    "    gen_img = torch.normal(MEAN, STD, size=(1, 1, 28, 28)).to(device)\n",
    "    gen_img.requires_grad = True\n",
    "\n",
    "    optim_adv = torch.optim.SGD([gen_img], lr=10**-2)\n",
    "\n",
    "    epochs = 500\n",
    "    cost_list = []\n",
    "    for i in range(epochs):\n",
    "        optim_adv.zero_grad()\n",
    "        C = -model_saved.forward(gen_img)[0, TARGET_CLASS]\n",
    "        cost_list.append(-C.to(torch.device('cpu')).detach().clone())\n",
    "        C.backward()\n",
    "        optim_adv.step()\n",
    "    cost_lists.append(cost_list)\n",
    "    gen_imgs.append(gen_img)\n",
    "\n",
    "    out = model_saved.forward(gen_img)\n",
    "    print(\"Probability\", nn.Softmax(dim=1)(out)[0][TARGET_CLASS])\n",
    "\n",
    "plt.figure(figsize = (20, 4))\n",
    "for cls in range(10):\n",
    "    plt.subplot(2, 5, cls + 1)\n",
    "    epochs = len(cost_lists[cls])\n",
    "    X = [i for i in range(epochs)]\n",
    "    Y = [cost_lists[cls][i] for i in range(epochs)]\n",
    "    plt.plot(X, Y)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Cost')\n",
    "# plt.show(); plt.clf()\n",
    "plt.savefig(img_dir + 'cost_vs_epochs_3_1.svg'); plt.clf()\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10, 3.5))\n",
    "for cls in range(10):\n",
    "    plt.subplot(2, 5, cls+1)\n",
    "    plt.imshow(gen_imgs[cls][0][0].to(torch.device('cpu')).detach(), cmap = 'gray');\n",
    "    plt.axis('off'); plt.ioff()\n",
    "    plt.title('Class ' + str(cls))\n",
    "# plt.show(); plt.clf()\n",
    "plt.savefig(img_dir + 'gen_images_3_1.svg'); plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da367f5",
   "metadata": {},
   "source": [
    "## 3.2 Targeted Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1aa78aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_CLASS = 3\n",
    "BETA = 10**-3\n",
    "ALPHA = 10**-1\n",
    "MEAN = 0.5\n",
    "STD = 0.1\n",
    "\n",
    "gen_imgs = []\n",
    "for ACTUAL_CLASS in range(10):\n",
    "    targetImage = getImage(ACTUAL_CLASS)\n",
    "\n",
    "    gen_img = torch.normal(MEAN, STD, size=(1, 1, 28, 28)).to(device)\n",
    "#     gen_img = targetImage.detach().clone().view(1, 1, 28, 28)\n",
    "    gen_img.requires_grad = True\n",
    "\n",
    "    optim_adv = torch.optim.SGD([gen_img], lr=ALPHA)\n",
    "    \n",
    "    pred_class = 11\n",
    "    epochs = 200\n",
    "    for i in range(epochs):\n",
    "#     while(pred_class != TARGET_CLASS):\n",
    "        optim_adv.zero_grad()\n",
    "        C = - (model_saved.forward(gen_img)[0, TARGET_CLASS] - BETA * nn.MSELoss()(gen_img[0], targetImage))\n",
    "        C.backward()\n",
    "        optim_adv.step()\n",
    "        pred_class = model_saved.forward(gen_img).argmax(1)\n",
    "\n",
    "    assert(pred_class == TARGET_CLASS)\n",
    "    gen_imgs.append(gen_img)\n",
    "\n",
    "plt.figure(figsize = (10, 3.5))\n",
    "for cls in range(10):\n",
    "    plt.subplot(2, 5, cls+1)\n",
    "    plt.imshow(gen_imgs[cls][0][0].to(torch.device('cpu')).detach(), cmap = 'gray');\n",
    "    plt.axis('off'); plt.ioff()\n",
    "# plt.show(); plt.clf()\n",
    "plt.savefig(img_dir + 'gen_images_3_2.svg'); plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b1c7cf",
   "metadata": {},
   "source": [
    "## Adding Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a80dbd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TARGET_CLASS = 3\n",
    "ALPHA = 10**-2\n",
    "\n",
    "gen_imgs = []\n",
    "noises = []\n",
    "for ACTUAL_CLASS in range(10):\n",
    "    targetImage = getImage(ACTUAL_CLASS)\n",
    "\n",
    "    noise = torch.zeros(1, 1, 28, 28).to(device)\n",
    "    noise.requires_grad = True\n",
    "\n",
    "    optim_adv = torch.optim.SGD([noise], lr=ALPHA)\n",
    "\n",
    "    pred_class = 11\n",
    "    while(pred_class != TARGET_CLASS):\n",
    "        optim_adv.zero_grad()\n",
    "        C = - (model_saved.forward(targetImage + noise)[0, TARGET_CLASS])\n",
    "        C.backward()\n",
    "        optim_adv.step()\n",
    "        pred_class = model_saved.forward(targetImage + noise).argmax(1)\n",
    "    gen_imgs.append(targetImage + noise)\n",
    "    noises.append(noise)\n",
    "    \n",
    "plt.figure(figsize = (12, 8))\n",
    "for cls in range(10):\n",
    "    plt.subplot(4, 6, 2*cls+1)\n",
    "    plt.imshow(gen_imgs[cls][0][0].to(torch.device('cpu')).detach(), cmap = 'gray');\n",
    "    plt.axis('off'); plt.ioff()\n",
    "    plt.title('Adv Image')\n",
    "    \n",
    "    plt.subplot(4, 6, 2*cls+2)\n",
    "    plt.imshow(noises[cls][0][0].to(torch.device('cpu')).detach(), cmap = 'gray');\n",
    "    plt.axis('off'); plt.ioff()\n",
    "    plt.title('Noise')\n",
    "# plt.show(); plt.clf()\n",
    "plt.savefig(img_dir + 'gen_images_3_3_1.svg'); plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d83a8299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTUAL CLASS = 0\n",
      "(noise 0 PRED CLASS = 0 ); (noise 1 PRED CLASS = 0 ); (noise 2 PRED CLASS = 0 ); \n",
      "(noise 3 PRED CLASS = 0 ); (noise 4 PRED CLASS = 0 ); (noise 5 PRED CLASS = 0 ); \n",
      "(noise 6 PRED CLASS = 0 ); (noise 7 PRED CLASS = 0 ); (noise 8 PRED CLASS = 0 ); \n",
      "(noise 9 PRED CLASS = 0 ); \n",
      "ACTUAL CLASS = 1\n",
      "(noise 0 PRED CLASS = 1 ); (noise 1 PRED CLASS = 3 ); (noise 2 PRED CLASS = 1 ); \n",
      "(noise 3 PRED CLASS = 1 ); (noise 4 PRED CLASS = 1 ); (noise 5 PRED CLASS = 1 ); \n",
      "(noise 6 PRED CLASS = 1 ); (noise 7 PRED CLASS = 1 ); (noise 8 PRED CLASS = 1 ); \n",
      "(noise 9 PRED CLASS = 1 ); \n",
      "ACTUAL CLASS = 2\n",
      "(noise 0 PRED CLASS = 2 ); (noise 1 PRED CLASS = 2 ); (noise 2 PRED CLASS = 2 ); \n",
      "(noise 3 PRED CLASS = 2 ); (noise 4 PRED CLASS = 2 ); (noise 5 PRED CLASS = 2 ); \n",
      "(noise 6 PRED CLASS = 2 ); (noise 7 PRED CLASS = 2 ); (noise 8 PRED CLASS = 2 ); \n",
      "(noise 9 PRED CLASS = 2 ); \n",
      "ACTUAL CLASS = 3\n",
      "(noise 0 PRED CLASS = 3 ); (noise 1 PRED CLASS = 3 ); (noise 2 PRED CLASS = 3 ); \n",
      "(noise 3 PRED CLASS = 3 ); (noise 4 PRED CLASS = 3 ); (noise 5 PRED CLASS = 3 ); \n",
      "(noise 6 PRED CLASS = 3 ); (noise 7 PRED CLASS = 3 ); (noise 8 PRED CLASS = 3 ); \n",
      "(noise 9 PRED CLASS = 3 ); \n",
      "ACTUAL CLASS = 4\n",
      "(noise 0 PRED CLASS = 4 ); (noise 1 PRED CLASS = 4 ); (noise 2 PRED CLASS = 4 ); \n",
      "(noise 3 PRED CLASS = 4 ); (noise 4 PRED CLASS = 4 ); (noise 5 PRED CLASS = 4 ); \n",
      "(noise 6 PRED CLASS = 7 ); (noise 7 PRED CLASS = 4 ); (noise 8 PRED CLASS = 4 ); \n",
      "(noise 9 PRED CLASS = 4 ); \n",
      "ACTUAL CLASS = 5\n",
      "(noise 0 PRED CLASS = 5 ); (noise 1 PRED CLASS = 5 ); (noise 2 PRED CLASS = 5 ); \n",
      "(noise 3 PRED CLASS = 5 ); (noise 4 PRED CLASS = 5 ); (noise 5 PRED CLASS = 5 ); \n",
      "(noise 6 PRED CLASS = 5 ); (noise 7 PRED CLASS = 5 ); (noise 8 PRED CLASS = 5 ); \n",
      "(noise 9 PRED CLASS = 5 ); \n",
      "ACTUAL CLASS = 6\n",
      "(noise 0 PRED CLASS = 6 ); (noise 1 PRED CLASS = 6 ); (noise 2 PRED CLASS = 6 ); \n",
      "(noise 3 PRED CLASS = 6 ); (noise 4 PRED CLASS = 6 ); (noise 5 PRED CLASS = 6 ); \n",
      "(noise 6 PRED CLASS = 6 ); (noise 7 PRED CLASS = 6 ); (noise 8 PRED CLASS = 6 ); \n",
      "(noise 9 PRED CLASS = 6 ); \n",
      "ACTUAL CLASS = 7\n",
      "(noise 0 PRED CLASS = 7 ); (noise 1 PRED CLASS = 7 ); (noise 2 PRED CLASS = 7 ); \n",
      "(noise 3 PRED CLASS = 7 ); (noise 4 PRED CLASS = 7 ); (noise 5 PRED CLASS = 7 ); \n",
      "(noise 6 PRED CLASS = 7 ); (noise 7 PRED CLASS = 7 ); (noise 8 PRED CLASS = 7 ); \n",
      "(noise 9 PRED CLASS = 7 ); \n",
      "ACTUAL CLASS = 8\n",
      "(noise 0 PRED CLASS = 8 ); (noise 1 PRED CLASS = 8 ); (noise 2 PRED CLASS = 8 ); \n",
      "(noise 3 PRED CLASS = 8 ); (noise 4 PRED CLASS = 8 ); (noise 5 PRED CLASS = 8 ); \n",
      "(noise 6 PRED CLASS = 8 ); (noise 7 PRED CLASS = 8 ); (noise 8 PRED CLASS = 8 ); \n",
      "(noise 9 PRED CLASS = 8 ); \n",
      "ACTUAL CLASS = 9\n",
      "(noise 0 PRED CLASS = 9 ); (noise 1 PRED CLASS = 9 ); (noise 2 PRED CLASS = 9 ); \n",
      "(noise 3 PRED CLASS = 9 ); (noise 4 PRED CLASS = 9 ); (noise 5 PRED CLASS = 9 ); \n",
      "(noise 6 PRED CLASS = 9 ); (noise 7 PRED CLASS = 9 ); (noise 8 PRED CLASS = 9 ); \n",
      "(noise 9 PRED CLASS = 9 ); \n"
     ]
    }
   ],
   "source": [
    "# res = []\n",
    "for ACTUAL_CLASS in range(10):\n",
    "    \n",
    "    img = None\n",
    "    print(\"ACTUAL CLASS =\", ACTUAL_CLASS)\n",
    "    for i in range(len(data_train)):\n",
    "        if(data_train[-(i+1)][1] == ACTUAL_CLASS):\n",
    "            img = data_train[-(i+1)][0].to(device)\n",
    "            break\n",
    "    \n",
    "    act_class = ACTUAL_CLASS\n",
    "    for idx, noise in enumerate(noises):\n",
    "        pred_class = int(model_saved.forward(img + noise).argmax(1).detach())\n",
    "#         res.append((act_class, pred_class))\n",
    "        print(\"(noise\", idx, \"PRED CLASS =\", pred_class, \")\", end=\"; \")\n",
    "        if(idx%3 == 2):\n",
    "            print()\n",
    "    print()\n",
    "# print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
