{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40cc9969",
   "metadata": {},
   "source": [
    "# Part 2 : Remebering Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf99f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "DEVICE_DEFAULT = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d751e6b9",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1680aaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pbar(p=0, msg=\"\", bar_len=20):\n",
    "    sys.stdout.write(\"\\033[K\")\n",
    "    sys.stdout.write(\"\\x1b[2K\" + \"\\r\")\n",
    "    block = int(round(bar_len * p))\n",
    "    text = \"Progress: [{}] {}% {}\".format(\n",
    "        \"\\x1b[32m\" + \"=\" * (block - 1) + \">\" + \"\\033[0m\" + \"-\" * (bar_len - block),\n",
    "        round(p * 100, 2),\n",
    "        msg,\n",
    "    )\n",
    "    print(text, end=\"\\r\")\n",
    "    if p == 1:\n",
    "        print()\n",
    "\n",
    "class AvgMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.metrics = {}\n",
    "\n",
    "    def add(self, batch_metrics):\n",
    "        for key, value in batch_metrics.items():\n",
    "            if key in self.metrics.items():\n",
    "                self.metrics[key].append(value)\n",
    "            else:\n",
    "                self.metrics[key] = [value]\n",
    "\n",
    "    def get(self):\n",
    "        return {key: np.mean(value) for key, value in self.metrics.items()}\n",
    "\n",
    "    def msg(self):\n",
    "        avg_metrics = {key: np.mean(value) for key, value in self.metrics.items()}\n",
    "        return \"\".join([\"[{}] {:.5f} \".format(key, value) for key, value in avg_metrics.items()])\n",
    "    \n",
    "def train(model, optim, lr_sched=None, epochs=200, device=DEVICE_DEFAULT, criterion=None, metric_meter=None, out_path=\"best.ckpt\"):\n",
    "    model.to(device)\n",
    "    best_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        metric_meter.reset()\n",
    "        for indx, (seq, target) in enumerate(train_data):\n",
    "            seq = seq.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            out = model.forward(seq)\n",
    "            loss = criterion(out, target)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            metric_meter.add({\"train loss\": loss.item()})\n",
    "#             pbar(indx / len(train_data), msg=metric_meter.msg())\n",
    "#         pbar(1, msg=metric_meter.msg())\n",
    "        train_loss_for_plot.append(metric_meter.get()[\"train loss\"])\n",
    "        print(train_loss_for_plot[-1], epoch)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        metric_meter.reset()\n",
    "        for indx, (seq, target) in enumerate(train_data):\n",
    "            seq = seq.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            out = model.forward(seq)\n",
    "            loss = criterion(out, target)\n",
    "            acc = (out.argmax(1) == target).sum().item() * (100 / seq.shape[0])\n",
    "            \n",
    "            metric_meter.add({\"train acc\": acc})\n",
    "#             pbar(indx / len(train_data), msg=metric_meter.msg())\n",
    "#         pbar(1, msg=metric_meter.msg())\n",
    "\n",
    "        train_metrics = metric_meter.get()\n",
    "        train_acc_for_plot.append(max(train_metrics[\"train acc\"], best_acc))\n",
    "        if train_metrics[\"train acc\"] > best_acc:\n",
    "            print(\n",
    "              \"\\x1b[33m\"\n",
    "              + f\"train acc improved from {round(best_acc, 5)} to {round(train_metrics['train acc'], 5)}\"\n",
    "              + \"\\033[0m\"\n",
    "            )\n",
    "            best_acc = train_metrics['train acc']\n",
    "#             torch.save(model.state_dict(), out_path)\n",
    "    lr_sched.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02352b8b",
   "metadata": {},
   "source": [
    "## Generate Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d54ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE, TEST_SIZE = 1000, 500\n",
    "\n",
    "# Generate a data point of given L\n",
    "# Output - (Seq of length L, One of Hot Rep of number at position pos)\n",
    "def gen_example(L, pos=2):\n",
    "    # (Batch S, seq S, feature S) = (1, L, 10)\n",
    "    assert L > pos\n",
    "    inp_before = torch.randint(0, 9 + 1, (3, L,))\n",
    "    inp = F.one_hot(inp_before, num_classes=10).type(torch.float)\n",
    "    out = inp_before[:,pos]\n",
    "    return (inp, out)\n",
    "\n",
    "data = []\n",
    "for i in range(TRAIN_SIZE + TEST_SIZE):\n",
    "    L = random.randint(3, 10+1)\n",
    "    data.append(gen_example(L))\n",
    "\n",
    "train_data, test_data = data[:TRAIN_SIZE], data[-TEST_SIZE:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccd3b38",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f0c3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, bidirectional, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = input_dim,\n",
    "                          hidden_size = hidden_dim,\n",
    "                          num_layers = num_layers,\n",
    "                          batch_first = True,\n",
    "                          bidirectional = bidirectional\n",
    "                         )\n",
    "        \n",
    "        D = (2 if bidirectional else 1)\n",
    "        \n",
    "        self.fc = nn.Linear(D * num_layers * hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        assert batch.dim() == 3\n",
    "        \n",
    "        output, (hidden, cell) = self.lstm(batch)\n",
    "        \n",
    "        # D = 2 if bidirectional, else D = 1\n",
    "        # output = [batch size, seq length, D * hidden_dim]\n",
    "        # hidden = [D * num_layers, batch size, hidden_dim]\n",
    "        \n",
    "        flat_hidden = torch.cat([hidden[i,:,:] for i in range(hidden.shape[0])], dim = 1)\n",
    "        \n",
    "        output = self.fc(flat_hidden)\n",
    "        \n",
    "        \n",
    "        return output\n",
    "\n",
    "# class RNN(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, num_layers, bidirectional, output_dim):\n",
    "        \n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.rnn = nn.RNN(input_size = input_dim,\n",
    "#                           hidden_size = hidden_dim,\n",
    "#                           num_layers = num_layers,\n",
    "#                           batch_first = True,\n",
    "#                           bidirectional = bidirectional,\n",
    "#                           nonlinearity = 'tanh'\n",
    "#                          )\n",
    "        \n",
    "#         D = (2 if bidirectional else 1)\n",
    "        \n",
    "#         self.fc = nn.Linear(D * num_layers * hidden_dim, output_dim)\n",
    "        \n",
    "#     def forward(self, batch):\n",
    "        \n",
    "#         assert batch.dim() == 3\n",
    "        \n",
    "#         output, hidden = self.rnn(batch)\n",
    "        \n",
    "#         # D = 2 if bidirectional, else D = 1\n",
    "#         # output = [batch size, seq length, D * hidden_dim]\n",
    "#         # hidden = [D * num_layers, batch size, hidden_dim]\n",
    "        \n",
    "#         flat_hidden = torch.cat([hidden[i,:,:] for i in range(hidden.shape[0])], dim = 1)\n",
    "\n",
    "#         output = self.fc(flat_hidden)\n",
    "        \n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc2ff13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = 10\n",
    "HIDDEN_DIM = 2\n",
    "OUTPUT_DIM = 10\n",
    "NUM_LAYERS = 1\n",
    "BIDIRECTIONAL = False\n",
    "EPOCHS = 200\n",
    "\n",
    "model = LSTM(INPUT_DIM, HIDDEN_DIM, NUM_LAYERS, BIDIRECTIONAL, OUTPUT_DIM)\n",
    "\n",
    "out_dir = \"Part2/\"\n",
    "out_path = out_dir + \"model2.ckpt\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# UNCOMMENT FROM HERE FOR TRAINING\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=10**-4, weight_decay=5e-4)\n",
    "# optim = torch.optim.SGD(model.parameters(), lr=10**-4, momentum=0.9)\n",
    "lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=EPOCHS)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "metric_meter = AvgMeter()\n",
    "\n",
    "train_loss_for_plot = []\n",
    "train_acc_for_plot = []\n",
    "\n",
    "train(model, optim, lr_sched, device=DEVICE_DEFAULT, epochs=EPOCHS, criterion=criterion, metric_meter=metric_meter, out_path=out_path)\n",
    "# After this the model will be saved in out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaab7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 3))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_loss_for_plot)\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Train Loss\")\n",
    "plt.title(\"Train Loss vs. Epochs\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_acc_for_plot)\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Train accuracy\")\n",
    "plt.title(\"Train Accuracy vs. Epochs\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6736c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = DEVICE_DEFAULT\n",
    "model.eval()\n",
    "metric_meter.reset()\n",
    "for indx, (seq, target) in enumerate(test_data):\n",
    "    seq = seq.to(device)\n",
    "    target = target.to(device)\n",
    "    out = model.forward(seq)\n",
    "#     loss = criterion(out, target)\n",
    "    acc = (out.argmax(1) == target).sum().item() * (100 / seq.shape[0])\n",
    "    metric_meter.add({\"test acc\": acc})\n",
    "\n",
    "test_metrics = metric_meter.get()\n",
    "print(\"Test Accuracy\", test_metrics[\"test acc\"], \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d047bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for L in range(3, 10 + 1):\n",
    "    for _ in range(2):\n",
    "        example_input = gen_example(L)\n",
    "        print(\"Input\", [example_input[0][0][i].argmax().item() for i in range(L)])\n",
    "        print(\"Received\", model.forward(example_input[0].to(device)).argmax(1).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
